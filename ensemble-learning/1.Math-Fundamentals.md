# Chapter 1: Mathematical Fundamentals of Machine Learning

1. Differential calculus

2. Linear algebra

3. Probability Theory and Statistics

4. Stochastic process and sampling principle

## 1. Differential calculus
1. Three matrix

**Gradient** of a differentiable function f is the vector field whose value at a point p is the vector whose components are the partial derivatives of f at p.

**Jacobian Matrix**: R_n->R_m. Every role is a gradient vector.

**Hessian Matrix**: Hessian matrix is the Jacobian Matrix of gradient vector.

2. Optimization
- Maxima and minima: f''(x0)<0-> Maxima, f''(x0)>0->Minima
- Saddle point: not good
- Multivariable: Hessian Matrix, positive-definite matrix->Minima:

positive-definite matrix: determinant>0 所有特征值为正

negative-definite matrix: determinant<0 所有特征值为负

3. Lagrange multiplier-equality constraint optimization problem

Add $/lamda$ to the optimization problem and take the derivatives of each variable and $/lamda$.

4. Tayler's theorem

5. [Gradient descent vs newton method](https://imlogm.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/gradientDescent/)

Gradient Descent:

```python
import numpy as np
import matplotlib.pyplot as plt


def f(x):
    return np.power(x, 2)

def d_f_2(f, x, delta=1e-4):
    return (f(x+delta) - f(x-delta)) / (2 * delta)

# plot the function
xs = np.arange(-10, 11)
plt.plot(xs, f(xs))

learning_rate = 0.1
max_loop = 30

x_init = 10.0
x = x_init
lr = 0.1
x_list = []
for i in range(max_loop):
    d_f_x = d_f_2(f, x)
    x = x - learning_rate * d_f_x
    x_list.append(x)
x_list = np.array(x_list)
plt.scatter(x_list,f(x_list),c="r")
plt.show()

print('initial x =', x_init)
print('arg min f(x) of x =', x)
print('f(x) =', f(x))
```
Newton's Method:



## 2. Linear Algebra
1. Vector space
2. Vector
- dot product
```python
import numpy as np
a = np.array([a1, a2, a3])
b = np.array([b1, b2, b3])
c = np.dot(np.transpose(a), b)
c = np.dot(a.T, b)
```
- Linearly dependent and linearly independent. n Linearly independent vectors can form a vector space.
- Schmidt process (格拉姆-施密特正交化)： 由线性无关向量组构建正交向量组
Orthogonality (正交)

```python
from sympy.matrices import Matrix, GramSchmidt
l = [Matrix([1,2,-1]), Matrix([-1,3,1]), Matrix([4,1,0])]
o = GramSchmidt(l, True)
o
```
4. Norm范数 -Measure distance
f(x)>=0 f(ax)=|a|f(x) f(a+c)<=f(a)+f(c)
```python
x = np.arange(12).reshape(3,4)
np.linalg.norm(x,ord=1) #norm 1
np.linalg.norm(x,ord=2) #norm 2
np.linalg.norm(x,ord=np.inf) #norm inf
```

5. Matrix

Ax = b
```python
# 写矩阵
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)]) #ndarray
B = np.mat(A) #转化成matrix datatype
m, n = 1,2
np.zeros((m, n)) #全零矩阵
np.ones((m, n)) #tuple #全1矩阵

# 矩阵加法
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
B = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
C = A + B

# 数乘
k = 10
k*A

# 矩阵乘法
np.matmul(A.T, B)

# 行列式
np.linalg.det(A)
```

- Determinant: 衡量了矩阵的变换剧烈程度

k of n independant vector

- Invertible matrix and othogonal matrix

- Matrix Norm

```python
# norm-1
x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
l1 = np.linalg.norm(x, ord = 1)
# norm-2
xtx = np.matmul(x.T, x)
lamda = np.linalg.eigvals(xtx)
np.sqrt(lamda[0])
l2 = np.linalg.norm(x, ord = 2)
# norm-info
l3 = np.linalg.norm(x, ord = np.inf)
# norm-fro
l4 = np.linalg.norm(x, ord = 'fro')
```

6. Rank

 ```
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 # 矩阵对秩
 np.linalg.matrix_rank(x)
 
 # 求解齐次线性方程组AX=b解空间:
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 b=np.ones((3,1))
 np.linalg.solve(x, b)
```

7. Eigenvalue and eigenvectors
- 矩阵变换之后方向不变

行列式是矩阵体积变化的幅度，迹是行列式变化的速度（行列式的导数）。

```
np.linalg.eig(x) #Eigenvectors
np.linalg.eigvals(x) #Eigenvalues

# Trace
trace = 0
for i in range(0, len(x)):
  trace = x[i,i] + trace
```

- Similar matrix

Eigenvector: Ax = /lamda x -> x^(-1)Ax = /lamda -> /lamda is a diagonal matrix

A and /lamda matrix are simiar matrix.

8. Quadratic form

A is a symmetric matrix. q(x)=x^T A x.

## 3. Probability 建模

0. Sample space: all sample result
1. Conditional probability
2. Independent: P(B|A)=P(B) P(AB)=P(A)P(B)
3. Law of total probability: 𝑃(𝐵)=𝑃(𝐴)𝑃(𝐵∣𝐴)+𝑃(𝐴¯)𝑃(𝐵∣𝐴¯) complete metric space
4. Bayes's theorem: P(A|B)={P(B|A)P(A)/P(B)

### Random Variable
1. Random Variable: random event(sample)->random variable
2. y=f(x)

p = P(random variable)

3. probability distribution: 
 
Discrete: bernoulli distribution, binomial distribution, Poisson distribution, Geometric distribution, Exponential distribution

Continuous: normal, uniform, chi-squared

| Distribution | Distribution Function PDF | E(X) | Var(X) |
| :---: | :---: | :---: | :---: |
| [Bernoulli Distribution](https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83) | f(X;p)=p^x*(1-p)^(1-x) | p | p(1-p) |
| [Binomial distribution](https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%A0%85%E5%BC%8F%E5%88%86%E5%B8%83) | f(X;n,p)=(n \choose i)p^i*(1-p)^(n-i) | np | np(1-p) |
| [Geometric distribution](https://zh.wikipedia.org/wiki/%E5%B9%BE%E4%BD%95%E5%88%86%E4%BD%88) | f(X;p) = (1 − p)^(x − 1)*p | 1/p | (1-p)/(p^2) |
| [Poisson Distribution](https://zh.wikipedia.org/wiki/%E5%8D%9C%E7%93%A6%E6%9D%BE%E5%88%86%E5%B8%83) | P(X=k;lamda)=\frac{e^{-\lamda}\lamda^k}{k!} | lamda | lamda |

| Distribution | Distribution Function PDF | E(X) | Var(X) |
| :---: | :---: | :---: | :---: |
| [Normal Distribution](https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83) | f(x;mu,sigma)=1/(sigma*sqrt(2\*pi))\*e^(-((x-mu)^2)/(2\*sigma^2)) | mu | sigma^2 |
| [Uniform Distribution](https://zh.wikipedia.org/wiki/%E9%80%A3%E7%BA%8C%E5%9E%8B%E5%9D%87%E5%8B%BB%E5%88%86%E5%B8%83) | f(x)=1/(b-a) | (a+b)/2 | (b-a)^2/12 |
| [Exponential distribution](https://zh.wikipedia.org/wiki/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83) | f(x;lamda)=lamda\*e^(-lamda\*x) | 1/lamda | 1/lamda^2 |
| [logarithmic normal distribution](https://zh.wikipedia.org/wiki/%E5%AF%B9%E6%95%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83) | f(x;mu,sigma)=1/(x*sigma*sqrt(2\*pi))\*e^(-((lnx-mu)^2)/(2\*sigma^2)) | e^(mu+sigma^2/2) | (e^(sigma^2)-1)*e^(2\*mu+sigma^2) |
| [chi-square distribution](https://zh.wikipedia.org/wiki/%E5%8D%A1%E6%96%B9%E5%88%86%E4%BD%88) | f(x;k)=1/(2^(k/2)\*Gamma(k/2))\*x^(k/2-1)\*e^(-x/2) | k | 2k |
| [t distribution](https://zh.wikipedia.org/wiki/%E5%8F%B8%E5%BE%92%E9%A0%93t%E5%88%86%E5%B8%83) |   |   |
| [f distribution](https://zh.wikipedia.org/wiki/F-%E5%88%86%E5%B8%83) |   |   |

Reference: https://zlearning.netlify.app/

4. Random vector(Multivariate random variable): Multinomial distribution(2->k), Multivariate normal distribution

joint distribution and marginal distribution

conditional distribution and independent

### Expectation and Variance (features) <- Density Function

1. Covariance=0 -> not correlated(degree of linearly related) -/> indpendent
2. Law of large number: sample mean convergents to ture expectation (assumption: n samples are independent and identically distributed)
3. Central Limit theorem: boosted sample mean convergents to normal distribution N(sqrt(n)u,sigma) (assumption: n samples are independent and identically distributed)


```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# n times sampling. Try N times.
def Sampling(N, n):
  rand_matrix = np.zeros((N, n))
  for i in range(N):
    rand_matrix[i:] = np.random.random(n)
  rand_mean = np.mean(rand_matrix, axis=1).reshape(N)
  sns.distplot(rand_mean, hist=True, kde=True, kde_kws={'color':'red'})
  return rand_mean

Sampling(100,10);
```

### Stochastic Process (random variable + time)

1. Stochastic Process: {𝑋𝑡|𝑡∈𝑇}, T∈(−∞,+∞). Time t is not a random variable. Given each t, we get a distribution. 
2. Markov Chain: 
3. Transition probability: 给定初始概率分布pi_0, pi_1=pi_0*P, ..., pi_n=pi_0*P^n
是否收敛->平稳->pi_i*P_ij = pi_j*P_ji

5. 



### Sample Simulation
 
## 4. Statistics 参数估计+假设检验

统计学研究不确定性，应用回归分析的方法，包括线性回归和机器学习。

1. 样本估计总体，样本直接独立，样本和总体同分布。参数：总体中未知但确定的量
2. 频率学派 Frequentist inference（参数是常数）点估计

Maximum Likelihood Estimation: Given observations, find the parameter values that maximize the probability of getting the observations.

Assumption: iid

max P(x1, x2, ...; para)=P(x1)P(x2)...=prod(P(xi))

lnP(x1, x2, ...)=sum(ln(P(xi)))

dlnP(x1, x2, ...)/d(para)=sum(dln(P(xi))/d(para))

概率：用于在已知参数的情况下，预测观测上所得到的结果；

似然性：在已知某些观测所得到的结果时，对有参数进行估值

5. 贝叶斯学派 Bayesians（参数是分布）：区间估计

- 贝叶斯估计：

小数据问题，容易出现overfitting问题->集成学习->贝叶斯模型（inf）

模型中融入不确定性：将前人经验利用

模型压缩

- 似然函数：

p(θ∣x)= p(x∣θ)p(θ)/p(x)

x：观察得到的数据（结果）

θ：决定数据分布的参数（原因）

p(θ)：先验 Prior probability

p(θ∣x)：后验 Posterior probability

p(x∣θ)：似然

p(x)：归一化因子

- MAP: maximum a posteriori estimation

θ* = argmax P(x∣θ)

- Bayes Estimation

E(θ)

p(θ∣x)= p(x∣θ)p(θ)/p(x)

待补充...估计和预测


