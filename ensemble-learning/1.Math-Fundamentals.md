# Chapter 1: Mathematical Fundamentals of Machine Learning

1. Differential calculus

2. Linear algebra

3. Probability Theory and Statistics

4. Stochastic process and sampling principle

## 1. Differential calculus
1. Three matrix

**Gradient** of a differentiable function f is the vector field whose value at a point p is the vector whose components are the partial derivatives of f at p.

**Jacobian Matrix**: R_n->R_m. Every role is a gradient vector.

**Hessian Matrix**: Hessian matrix is the Jacobian Matrix of gradient vector.

2. Optimization
- Maxima and minima: f''(x0)<0-> Maxima, f''(x0)>0->Minima
- Saddle point: not good
- Multivariable: Hessian Matrix, positive-definite matrix->Minima:

positive-definite matrix: determinant>0 所有特征值为正

negative-definite matrix: determinant<0 所有特征值为负

3. Lagrange multiplier-equality constraint optimization problem

Add $/lamda$ to the optimization problem and take the derivatives of each variable and $/lamda$.

4. Tayler's theorem

5. [Gradient descent vs newton method](https://imlogm.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/gradientDescent/)

Gradient Descent:

```python
import numpy as np
import matplotlib.pyplot as plt


def f(x):
    return np.power(x, 2)

def d_f_2(f, x, delta=1e-4):
    return (f(x+delta) - f(x-delta)) / (2 * delta)

# plot the function
xs = np.arange(-10, 11)
plt.plot(xs, f(xs))

learning_rate = 0.1
max_loop = 30

x_init = 10.0
x = x_init
lr = 0.1
x_list = []
for i in range(max_loop):
    d_f_x = d_f_2(f, x)
    x = x - learning_rate * d_f_x
    x_list.append(x)
x_list = np.array(x_list)
plt.scatter(x_list,f(x_list),c="r")
plt.show()

print('initial x =', x_init)
print('arg min f(x) of x =', x)
print('f(x) =', f(x))
```
Newton's Method:



## 2. Linear Algebra
1. Vector space
2. Vector
- dot product
```python
import numpy as np
a = np.array([a1, a2, a3])
b = np.array([b1, b2, b3])
c = np.dot(np.transpose(a), b)
c = np.dot(a.T, b)
```
- Linearly dependent and linearly independent. n Linearly independent vectors can form a vector space.
- Schmidt process (格拉姆-施密特正交化)： 由线性无关向量组构建正交向量组
Orthogonality (正交)

```python
from sympy.matrices import Matrix, GramSchmidt
l = [Matrix([1,2,-1]), Matrix([-1,3,1]), Matrix([4,1,0])]
o = GramSchmidt(l, True)
o
```
4. Norm范数 -Measure distance
f(x)>=0 f(ax)=|a|f(x) f(a+c)<=f(a)+f(c)
```python
x = np.arange(12).reshape(3,4)
np.linalg.norm(x,ord=1) #norm 1
np.linalg.norm(x,ord=2) #norm 2
np.linalg.norm(x,ord=np.inf) #norm inf
```

5. Matrix

Ax = b
```python
# 写矩阵
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)]) #ndarray
B = np.mat(A) #转化成matrix datatype
m, n = 1,2
np.zeros((m, n)) #全零矩阵
np.ones((m, n)) #tuple #全1矩阵

# 矩阵加法
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
B = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
C = A + B

# 数乘
k = 10
k*A

# 矩阵乘法
np.matmul(A.T, B)

# 行列式
np.linalg.det(A)
```

- Determinant: 衡量了矩阵的变换剧烈程度

k of n independant vector

- Invertible matrix and othogonal matrix

- Matrix Norm

```python
# norm-1
x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
l1 = np.linalg.norm(x, ord = 1)
# norm-2
xtx = np.matmul(x.T, x)
lamda = np.linalg.eigvals(xtx)
np.sqrt(lamda[0])
l2 = np.linalg.norm(x, ord = 2)
# norm-info
l3 = np.linalg.norm(x, ord = np.inf)
# norm-fro
l4 = np.linalg.norm(x, ord = 'fro')
```

6. Rank

 ```
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 # 矩阵对秩
 np.linalg.matrix_rank(x)
 
 # 求解齐次线性方程组AX=b解空间:
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 b=np.ones((3,1))
 np.linalg.solve(x, b)
```

7. Eigenvalue and eigenvectors
- 矩阵变换之后方向不变

行列式是矩阵体积变化的幅度，迹是行列式变化的速度（行列式的导数）。

```
np.linalg.eig(x) #Eigenvectors
np.linalg.eigvals(x) #Eigenvalues

# Trace
trace = 0
for i in range(0, len(x)):
  trace = x[i,i] + trace
```

- Similar matrix

Eigenvector: Ax = /lamda x -> x^(-1)Ax = /lamda -> /lamda is a diagonal matrix

A and /lamda matrix are simiar matrix.

8. Quadratic form

A is a symmetric matrix. q(x)=x^T A x.










