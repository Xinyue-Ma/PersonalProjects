# Chapter 1: Mathematical Fundamentals of Machine Learning

1. Differential calculus

2. Linear algebra

3. Probability Theory and Statistics

4. Stochastic process and sampling principle

## 1. Differential calculus
1. Three matrix

**Gradient** of a differentiable function f is the vector field whose value at a point p is the vector whose components are the partial derivatives of f at p.

**Jacobian Matrix**: R_n->R_m. Every role is a gradient vector.

**Hessian Matrix**: Hessian matrix is the Jacobian Matrix of gradient vector.

2. Optimization
- Maxima and minima: f''(x0)<0-> Maxima, f''(x0)>0->Minima
- Saddle point: not good
- Multivariable: Hessian Matrix, positive-definite matrix->Minima:

positive-definite matrix: determinant>0 æ‰€æœ‰ç‰¹å¾å€¼ä¸ºæ­£

negative-definite matrix: determinant<0 æ‰€æœ‰ç‰¹å¾å€¼ä¸ºè´Ÿ

3. Lagrange multiplier-equality constraint optimization problem

Add $/lamda$ to the optimization problem and take the derivatives of each variable and $/lamda$.

4. Tayler's theorem

5. [Gradient descent vs newton method](https://imlogm.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/gradientDescent/)

Gradient Descent:

```python
import numpy as np
import matplotlib.pyplot as plt


def f(x):
    return np.power(x, 2)

def d_f_2(f, x, delta=1e-4):
    return (f(x+delta) - f(x-delta)) / (2 * delta)

# plot the function
xs = np.arange(-10, 11)
plt.plot(xs, f(xs))

learning_rate = 0.1
max_loop = 30

x_init = 10.0
x = x_init
lr = 0.1
x_list = []
for i in range(max_loop):
    d_f_x = d_f_2(f, x)
    x = x - learning_rate * d_f_x
    x_list.append(x)
x_list = np.array(x_list)
plt.scatter(x_list,f(x_list),c="r")
plt.show()

print('initial x =', x_init)
print('arg min f(x) of x =', x)
print('f(x) =', f(x))
```
Newton's Method:



## 2. Linear Algebra
1. Vector space
2. Vector
- dot product
```python
import numpy as np
a = np.array([a1, a2, a3])
b = np.array([b1, b2, b3])
c = np.dot(np.transpose(a), b)
c = np.dot(a.T, b)
```
- Linearly dependent and linearly independent. n Linearly independent vectors can form a vector space.
- Schmidt process (æ ¼æ‹‰å§†-æ–½å¯†ç‰¹æ­£äº¤åŒ–)ï¼š ç”±çº¿æ€§æ— å…³å‘é‡ç»„æ„å»ºæ­£äº¤å‘é‡ç»„
Orthogonality (æ­£äº¤)

```python
from sympy.matrices import Matrix, GramSchmidt
l = [Matrix([1,2,-1]), Matrix([-1,3,1]), Matrix([4,1,0])]
o = GramSchmidt(l, True)
o
```
4. NormèŒƒæ•° -Measure distance
f(x)>=0 f(ax)=|a|f(x) f(a+c)<=f(a)+f(c)
```python
x = np.arange(12).reshape(3,4)
np.linalg.norm(x,ord=1) #norm 1
np.linalg.norm(x,ord=2) #norm 2
np.linalg.norm(x,ord=np.inf) #norm inf
```

5. Matrix

Ax = b
```python
# å†™çŸ©é˜µ
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)]) #ndarray
B = np.mat(A) #è½¬åŒ–æˆmatrix datatype
m, n = 1,2
np.zeros((m, n)) #å…¨é›¶çŸ©é˜µ
np.ones((m, n)) #tuple #å…¨1çŸ©é˜µ

# çŸ©é˜µåŠ æ³•
A = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
B = np.array([(1,2,3,4),(5,6,7,8),(9,10,11,12)])
C = A + B

# æ•°ä¹˜
k = 10
k*A

# çŸ©é˜µä¹˜æ³•
np.matmul(A.T, B)

# è¡Œåˆ—å¼
np.linalg.det(A)
```

- Determinant: è¡¡é‡äº†çŸ©é˜µçš„å˜æ¢å‰§çƒˆç¨‹åº¦

k of n independant vector

- Invertible matrix and othogonal matrix

- Matrix Norm

```python
# norm-1
x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
l1 = np.linalg.norm(x, ord = 1)
# norm-2
xtx = np.matmul(x.T, x)
lamda = np.linalg.eigvals(xtx)
np.sqrt(lamda[0])
l2 = np.linalg.norm(x, ord = 2)
# norm-info
l3 = np.linalg.norm(x, ord = np.inf)
# norm-fro
l4 = np.linalg.norm(x, ord = 'fro')
```

6. Rank

 ```
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 # çŸ©é˜µå¯¹ç§©
 np.linalg.matrix_rank(x)
 
 # æ±‚è§£é½æ¬¡çº¿æ€§æ–¹ç¨‹ç»„AX=bè§£ç©ºé—´:
 x = np.array([[-1, 1, 0],[-4, 3, 0],[1,0, 2]])
 b=np.ones((3,1))
 np.linalg.solve(x, b)
```

7. Eigenvalue and eigenvectors
- çŸ©é˜µå˜æ¢ä¹‹åæ–¹å‘ä¸å˜

è¡Œåˆ—å¼æ˜¯çŸ©é˜µä½“ç§¯å˜åŒ–çš„å¹…åº¦ï¼Œè¿¹æ˜¯è¡Œåˆ—å¼å˜åŒ–çš„é€Ÿåº¦ï¼ˆè¡Œåˆ—å¼çš„å¯¼æ•°ï¼‰ã€‚

```
np.linalg.eig(x) #Eigenvectors
np.linalg.eigvals(x) #Eigenvalues

# Trace
trace = 0
for i in range(0, len(x)):
  trace = x[i,i] + trace
```

- Similar matrix

Eigenvector: Ax = /lamda x -> x^(-1)Ax = /lamda -> /lamda is a diagonal matrix

A and /lamda matrix are simiar matrix.

8. Quadratic form

A is a symmetric matrix. q(x)=x^T A x.

## 3. Probability å»ºæ¨¡

0. Sample space: all sample result
1. Conditional probability
2. Independent: P(B|A)=P(B) P(AB)=P(A)P(B)
3. Law of total probability: ğ‘ƒ(ğµ)=ğ‘ƒ(ğ´)ğ‘ƒ(ğµâˆ£ğ´)+ğ‘ƒ(ğ´Â¯)ğ‘ƒ(ğµâˆ£ğ´Â¯) complete metric space
4. Bayes's theorem: P(A|B)={P(B|A)P(A)/P(B)

### Random Variable
1. Random Variable: random event(sample)->random variable
2. y=f(x)

p = P(random variable)

3. probability distribution: 
 
Discrete: bernoulli distribution, binomial distribution, Poisson distribution, Geometric distribution, Exponential distribution

Continuous: normal, uniform, chi-squared

 | Distribution | Probability mass function/PDF | E(X) | Var(X) |
 | --- | --- | --- | --- | --- |
 | [Bernoulli Distribution](https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83) | f(X)=p^x*(1-p)^(1-x) | E(X)=p | Var(X)=p(q-p) |

| è¡¨æ ¼      | ç¬¬ä¸€åˆ—     | ç¬¬äºŒåˆ—     |
| ---------- | :-----------:  | :-----------: |
| ç¬¬ä¸€è¡Œ     | ç¬¬ä¸€åˆ—     | ç¬¬äºŒåˆ—     |


4. Random vector(Multivariate random variable): Multinomial distribution(2->k), Multivariate normal distribution

joint distribution and marginal distribution

conditional distribution and independent

### Expectation and Variance (features) <- Density Function

1. Covariance=0 -> not correlated(degree of linearly related) -/> indpendent
2. Law of large number: sample mean convergents to ture expectation (assumption: n samples are independent and identically distributed)
3. Central Limit theorem: boosted sample mean convergents to normal distribution N(sqrt(n)u,sigma) (assumption: n samples are independent and identically distributed)

### Stochastic Process (random variable + time)

### Sample Simulation



 
## 4. Statistics å‚æ•°ä¼°è®¡+å‡è®¾æ£€éªŒ

ç»Ÿè®¡å­¦ç ”ç©¶ä¸ç¡®å®šæ€§ï¼Œåº”ç”¨å›å½’åˆ†æçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬çº¿æ€§å›å½’å’Œæœºå™¨å­¦ä¹ ã€‚







