# Classification Problem

1. 分类是预测一个离散标签的任务，
回归是预测一个连续数量的任务

2. 在使用最小均方差Loss时，模型参数w会学习的非常慢。而使用交叉熵Loss则没有这个问题。为了更快的学习速度，分类问题一般采用交叉熵损失函数

3. LDA根据样本计算均值和协方差矩阵，然后带入判别式。LogitsR使用极大对数似然估计参数

5. LDA: 分类的数据边界比较明显，或者说类别之间的距离比较，这种情况系啊，logistic regression会非常的不稳定， 但是LDA不会收到影响； 
各个参量的X取值符合正态分布（这里不是必要满足）我们只需要发现特征当中的某种规律即可，但是一般情况下，我们都认为是正态分布。即便n很小，模型也能得到不错的拟合效果（数据逼近真实正态不需要多少的采样点）； 
LDA在多分类问题上，和二分类模型基本没差距，效果差距不大； 
LDA可以完成降维，完成数据可以进行可视化。

QDA通过解除了LDA的限定条件增加了模型的复杂度。对于线性情况，二者差距不是很大，QDA虽然稍有弯曲，但是对于大的分类效果没有很大的影。

贝叶斯最主要的适用范围是特征量p很大，而且n比较少的情况下。LDA和QDA在这种情况都无法达到很好的效果。
