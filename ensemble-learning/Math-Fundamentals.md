# Chapter 1: Mathematical Fundamentals of Machine Learning

1. Differential calculus

2. Linear algebra

3. Probability Theory and Statistics

4. Stochastic process and sampling principle

## 1. Differential calculus
'gradient of a scalar-valued differentiable function f of several variables is the vector field (or vector-valued function) {\displaystyle \nabla f}\nabla f whose value at a point {\displaystyle p}p is the vector[a] whose components are the partial derivatives of {\displaystyle f}f at {\displaystyle p}p.[1][2][3][4][5][6][7][8][9] That is, for {\displaystyle f\colon \mathbb {R} ^{n}\to \mathbb {R} }{\displaystyle f\colon \mathbb {R} ^{n}\to \mathbb {R} }, its gradient {\displaystyle \nabla f\colon \mathbb {R} ^{n}\to \mathbb {R} ^{n}}{\displaystyle \nabla f\colon \mathbb {R} ^{n}\to \mathbb {R} ^{n}} is defined at the point {\displaystyle p=(x_{1},\ldots ,x_{n})}{\displaystyle p=(x_{1},\ldots ,x_{n})} in n-dimensional space as the vector:[b]

{\displaystyle \nabla f(p)={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}.}{\displaystyle \nabla f(p)={\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}}.}'


