{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly draw ten 8-Ks per quarter from 1995-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this. Take lots of time.\n",
    "import edgar\n",
    "\n",
    "download_directory = 'tsv/'\n",
    "since_year = 1995\n",
    "\n",
    "# The SEC has a rate request limit. So we run several times to make sure we get all data we need.\n",
    "for i in range(10):\n",
    "    edgar.download_index(download_directory, since_year, skip_all_present_except_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create name for all files\n",
    "QTR = ['QTR1','QTR2','QTR3','QTR4']\n",
    "path = 'tsv/'\n",
    "file_list = []\n",
    "for i in range(1995,2021) :\n",
    "    for j in QTR:\n",
    "        file_list.append(path + str(i) + '-' + j + '.tsv')\n",
    "        \n",
    "# Initialize record\n",
    "record = pd.DataFrame(columns = ['CIK', 'Name', 'Form', 'Date', 'TXT', 'Link'])\n",
    "        \n",
    "# Read in and process files\n",
    "for i in file_list:\n",
    "    df = pd.read_csv(i, sep = '|', error_bad_lines=False, header=None)\n",
    "    df.columns = ['CIK', 'Name', 'Form', 'Date', 'TXT', 'Link']\n",
    "    # Sample 100 for each year and quarter\n",
    "    df_sample = df[df['Form']==\"8-K\"].sample(10)\n",
    "    # Record\n",
    "    record = pd.concat([record, df_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>Name</th>\n",
       "      <th>Form</th>\n",
       "      <th>Date</th>\n",
       "      <th>TXT</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216228</td>\n",
       "      <td>ITT CORP</td>\n",
       "      <td>8-K</td>\n",
       "      <td>1995-02-06</td>\n",
       "      <td>edgar/data/216228/0000950123-95-000211.txt</td>\n",
       "      <td>edgar/data/216228/0000950123-95-000211-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723916</td>\n",
       "      <td>MERIDIAN BANCORP INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>1995-01-12</td>\n",
       "      <td>edgar/data/723916/0000903594-95-000001.txt</td>\n",
       "      <td>edgar/data/723916/0000903594-95-000001-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>846972</td>\n",
       "      <td>ADIENCE INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>edgar/data/846972/0000846972-95-000004.txt</td>\n",
       "      <td>edgar/data/846972/0000846972-95-000004-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853890</td>\n",
       "      <td>KANEB PIPE LINE PARTNERS L P</td>\n",
       "      <td>8-K</td>\n",
       "      <td>1995-03-13</td>\n",
       "      <td>edgar/data/853890/0000950109-95-000665.txt</td>\n",
       "      <td>edgar/data/853890/0000950109-95-000665-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>790070</td>\n",
       "      <td>EMC CORP</td>\n",
       "      <td>8-K</td>\n",
       "      <td>1995-03-03</td>\n",
       "      <td>edgar/data/790070/0000790070-95-000006.txt</td>\n",
       "      <td>edgar/data/790070/0000790070-95-000006-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>854800</td>\n",
       "      <td>MICT, Inc.</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>edgar/data/854800/0001213900-20-030486.txt</td>\n",
       "      <td>edgar/data/854800/0001213900-20-030486-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>948320</td>\n",
       "      <td>CONVERSION LABS, INC.</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>edgar/data/948320/0001493152-20-022570.txt</td>\n",
       "      <td>edgar/data/948320/0001493152-20-022570-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>1693577</td>\n",
       "      <td>MainStreet Bancshares, Inc.</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>edgar/data/1693577/0001564590-20-054603.txt</td>\n",
       "      <td>edgar/data/1693577/0001564590-20-054603-index....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1142596</td>\n",
       "      <td>NUVASIVE INC</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>edgar/data/1142596/0001564590-20-048859.txt</td>\n",
       "      <td>edgar/data/1142596/0001564590-20-048859-index....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>1304409</td>\n",
       "      <td>ATHENA SILVER CORP</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2020-10-28</td>\n",
       "      <td>edgar/data/1304409/0001683168-20-003575.txt</td>\n",
       "      <td>edgar/data/1304409/0001683168-20-003575-index....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1040 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CIK                          Name Form        Date  \\\n",
       "0      216228                      ITT CORP  8-K  1995-02-06   \n",
       "1      723916          MERIDIAN BANCORP INC  8-K  1995-01-12   \n",
       "2      846972                   ADIENCE INC  8-K  1995-02-10   \n",
       "3      853890  KANEB PIPE LINE PARTNERS L P  8-K  1995-03-13   \n",
       "4      790070                      EMC CORP  8-K  1995-03-03   \n",
       "...       ...                           ...  ...         ...   \n",
       "1035   854800                    MICT, Inc.  8-K  2020-10-07   \n",
       "1036   948320         CONVERSION LABS, INC.  8-K  2020-11-25   \n",
       "1037  1693577   MainStreet Bancshares, Inc.  8-K  2020-11-19   \n",
       "1038  1142596                  NUVASIVE INC  8-K  2020-10-29   \n",
       "1039  1304409            ATHENA SILVER CORP  8-K  2020-10-28   \n",
       "\n",
       "                                              TXT  \\\n",
       "0      edgar/data/216228/0000950123-95-000211.txt   \n",
       "1      edgar/data/723916/0000903594-95-000001.txt   \n",
       "2      edgar/data/846972/0000846972-95-000004.txt   \n",
       "3      edgar/data/853890/0000950109-95-000665.txt   \n",
       "4      edgar/data/790070/0000790070-95-000006.txt   \n",
       "...                                           ...   \n",
       "1035   edgar/data/854800/0001213900-20-030486.txt   \n",
       "1036   edgar/data/948320/0001493152-20-022570.txt   \n",
       "1037  edgar/data/1693577/0001564590-20-054603.txt   \n",
       "1038  edgar/data/1142596/0001564590-20-048859.txt   \n",
       "1039  edgar/data/1304409/0001683168-20-003575.txt   \n",
       "\n",
       "                                                   Link  \n",
       "0     edgar/data/216228/0000950123-95-000211-index.html  \n",
       "1     edgar/data/723916/0000903594-95-000001-index.html  \n",
       "2     edgar/data/846972/0000846972-95-000004-index.html  \n",
       "3     edgar/data/853890/0000950109-95-000665-index.html  \n",
       "4     edgar/data/790070/0000790070-95-000006-index.html  \n",
       "...                                                 ...  \n",
       "1035  edgar/data/854800/0001213900-20-030486-index.html  \n",
       "1036  edgar/data/948320/0001493152-20-022570-index.html  \n",
       "1037  edgar/data/1693577/0001564590-20-054603-index....  \n",
       "1038  edgar/data/1142596/0001564590-20-048859-index....  \n",
       "1039  edgar/data/1304409/0001683168-20-003575-index....  \n",
       "\n",
       "[1040 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record.to_csv(\"record.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 8-K files. Download will take lots of time. Don't run this.\n",
    "files = []\n",
    "\n",
    "for index, value in record.iterrows():\n",
    "    url = 'https://www.sec.gov/Archives/' + value['TXT']\n",
    "    # print(url)\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    while str(r) != '<Response [200]>': # <Response [200]> means it works fine.\n",
    "        r = requests.get(url, allow_redirects=True) # Always make sure we get the valid text data\n",
    "    path = '8-K/' + str(value['CIK']) + '-' + value['Date'] + '.txt' \n",
    "    open(path, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 8-K files into files\n",
    "record = pd.read_csv('record.csv')\n",
    "files = []\n",
    "for index, value in record.iterrows():\n",
    "    path = '8-K/' + str(value['CIK']) + '-' + value['Date'] + '.txt'\n",
    "    f = open(path,'r')\n",
    "    file = f.read()\n",
    "    files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all 8-K filings with Item\n",
    "def paragraph_split(file):\n",
    "    num = 0\n",
    "    list_ret = list()\n",
    "    if '</IMS-HEADER>' in file: # old\n",
    "        soup = BeautifulSoup(file, 'lxml').find('text')\n",
    "        temp = str(soup).lower().split('signature')[0]\n",
    "        temp = temp.lower().split('item 9.01')[0]\n",
    "    elif '</SEC-HEADER>' in file: # new\n",
    "        soup = BeautifulSoup(file, 'lxml').find_all('text')[0]\n",
    "        temp = soup.get_text().lower().split('item 9.01')[0]\n",
    "        temp = temp.lower().split('signature')[0]\n",
    "    else: \n",
    "        num = num+1\n",
    "    for s_str in temp.lower().split('item')[1:]:\n",
    "        s_str = s_str.replace(\"\\xa0\", \"\")\n",
    "#         s_str = s_str.replace(\"\\n\", \" \")\n",
    "        list_ret.append(s_str)\n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add all itmes to the items\n",
    "items = list()\n",
    "for file in files:\n",
    "    for s_str in paragraph_split(file):\n",
    "        s_str = s_str.replace(\"\\n\", \"\") # Remove '\\n'\n",
    "        s_str = s_str.replace(\"  \", \" \")\n",
    "        items.append(s_str)\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data into excel\n",
    "items_df = pd.DataFrame(items)\n",
    "items_df.to_csv('items.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2575"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import torch\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# from ey_nlp.contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from IPython import embed\n",
    "#import ipdb\n",
    "import time\n",
    "import pickle\n",
    "from spacy.lang.en import English # updated\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xinyue/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# from the paper\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def remove_proper_nouns(text = 'I am named John Dow'):\n",
    "    # written by Robert Hatem\n",
    "    text_tagged = nltk.tag.pos_tag(text.split())\n",
    "    text_edited = [word for word, tag in text_tagged if tag != 'NNP' and tag != 'NNPS']\n",
    "    text_new = ' '.join(text_edited)\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra preprocessing\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_extra_newlines(text):\n",
    "    return re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text)\n",
    "    \n",
    "def remove_extra_whitespace(text):\n",
    "    return re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for customer tockenizer\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc,\n",
    "                    proper_noun_removal=False,\n",
    "                    lower_the_case=True,\n",
    "                    special_char_removal=True,\n",
    "                    remove_digits=True,\n",
    "                    stopword_removal=True,\n",
    "                    html_stripping=True,\n",
    "                    accented_char_removal=True):\n",
    "    \"\"\"\n",
    "    doc: string\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove proper nouns\n",
    "    if proper_noun_removal:\n",
    "        doc = remove_proper_nouns(doc)\n",
    "    # lowercase the text    \n",
    "    if lower_the_case:\n",
    "        doc = lower_case(doc)\n",
    "    # expand contractions    \n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        doc = remove_stopwords(doc, is_lower_case=lower_the_case)\n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        doc = remove_accented_chars(doc)\n",
    "    doc = remove_extra_newlines(doc)\n",
    "    doc = remove_extra_whitespace(doc)\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(doc,\n",
    "                     stemmer=True,\n",
    "                     text_lemmatization=True):\n",
    "    # stemmer\n",
    "    if stemmer:\n",
    "        doc = simple_stemmer(doc)\n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        doc = lemmatize_text(doc)\n",
    "    return doc.split()\n",
    "\n",
    "\n",
    "def clean_text(doc):\n",
    "    \"\"\"\n",
    "    Clean then tockenize data\n",
    "    \"\"\"\n",
    "    doc_clean = preprocess_text(doc)\n",
    "    doc_clean = custom_tokenizer(doc_clean)\n",
    "    \n",
    "    doc_as_string = ' '.join(doc_clean)\n",
    "    return doc_as_string\n",
    "\n",
    "\n",
    "# DEPRECATED\n",
    "def _count_words(corpus):\n",
    "    '''\n",
    "    Makes document-term matrix  \n",
    "    corpus: list of strings\n",
    "    '''\n",
    "    \n",
    "    if type(corpus) != list:\n",
    "        raise TypeError('corpus should be list or nltk corpus')\n",
    "        \n",
    "    vectorizer = CountVectorizer(preprocessor=preprocess_text,\n",
    "                                 tokenizer=custom_tokenizer)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    doc_term_mat = X.toarray()\n",
    "\n",
    "    return vocab, doc_term_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_cleaned = [clean_text(item) for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data into excel\n",
    "items_cleaned_df = pd.DataFrame(items_cleaned)\n",
    "items_cleaned_df.to_csv('items_cleaned.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_csv('labeled.csv', encoding = \"ISO-8859-1\")[0:10][['event', 'labels']]\n",
    "labeled_cleaned = [clean_text(item) for item in labeled['event']]\n",
    "labeled_cleaned_df = pd.DataFrame(labeled_cleaned, labeled['labels'])\n",
    "labeled_cleaned_df.to_csv('labeled_cleaned.csv',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
